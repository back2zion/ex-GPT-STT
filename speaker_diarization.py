#!/usr/bin/env python3
"""
í™”ì ë¶„ë¦¬(Speaker Diarization) ê¸°ëŠ¥
pyannote.audioë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì êµ¬ë¶„
"""

import os
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

def perform_speaker_diarization(audio_file, num_speakers=None):
    """
    ì‹¤ì œ í™”ì ë¶„ë¦¬ ìˆ˜í–‰
    
    Args:
        audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        num_speakers (int, optional): ì˜ˆìƒ í™”ì ìˆ˜ (Noneì´ë©´ ìë™ ê°ì§€)
    
    Returns:
        dict: í™”ìë³„ ì‹œê°„ êµ¬ê°„ ì •ë³´
    """
    try:
        from pyannote.audio import Pipeline
        import torch
        
        print("ğŸ­ ì‹¤ì œ ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹œì‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ í™”ì ë¶„ë¦¬ ë””ë°”ì´ìŠ¤: {device}")
        
        # pyannote íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ“¥ pyannote.audio ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # Hugging Face í† í° ì„¤ì • (ì—¬ëŸ¬ ë°©ë²• ì§€ì›)
        hf_token = None
        
        # 1. í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸°
        hf_token = os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
        
        # 2. í† í°ì´ ì—†ìœ¼ë©´ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
        if not hf_token:
            print("âš ï¸ Hugging Face í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì„¤ì • ë°©ë²•:")
            print("   export HF_TOKEN='your_token_here'")
            print("   ë˜ëŠ” huggingface-cli login")
            return None
        
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
        except Exception as e:
            if "token" in str(e).lower() or "authentication" in str(e).lower():
                print(f"âŒ ì¸ì¦ ì˜¤ë¥˜: {e}")
                print("ğŸ’¡ í•´ê²° ë°©ë²•:")
                print("   1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •: export HF_TOKEN='your_token_here'")
                print("   2. ë˜ëŠ” huggingface-cli login ì‹¤í–‰")
                raise
            else:
                raise
        
        if device == "cuda":
            pipeline = pipeline.to(torch.device("cuda"))
        
        # í™”ì ë¶„ë¦¬ ì‹¤í–‰
        print(f"ğŸ¯ í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘... (ì˜ˆìƒ í™”ì ìˆ˜: {num_speakers or 'ìë™ê°ì§€'})")
        
        diarization_params = {}
        if num_speakers:
            diarization_params["num_speakers"] = num_speakers
        
        diarization = pipeline(audio_file, **diarization_params)
        
        # ê²°ê³¼ ì²˜ë¦¬
        speaker_segments = {}
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speaker_id = f"í™”ì{speaker.split('_')[-1] if '_' in speaker else speaker}"
            
            if speaker_id not in speaker_segments:
                speaker_segments[speaker_id] = []
            
            speaker_segments[speaker_id].append({
                'start': turn.start,
                'end': turn.end,
                'duration': turn.end - turn.start
            })
        
        print(f"âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ! ê°ì§€ëœ í™”ì ìˆ˜: {len(speaker_segments)}")
        
        # í™”ìë³„ í†µê³„
        for speaker, segments in speaker_segments.items():
            total_time = sum(seg['duration'] for seg in segments)
            print(f"   ğŸ“Š {speaker}: {len(segments)}ê°œ êµ¬ê°„, ì´ {total_time:.1f}ì´ˆ")
        
        return speaker_segments
        
    except ImportError:
        print("âš ï¸ pyannote.audioê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì‹¤í–‰ ëª…ë ¹: uv sync --extra diarization")
        return None
        
    except Exception as e:
        print(f"âŒ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {str(e)}")
        if "token" in str(e).lower():
            print("ğŸ’¡ Hugging Face í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("   1. https://huggingface.co/settings/tokens ì—ì„œ í† í° ìƒì„±")
            print("   2. huggingface-cli login ì‹¤í–‰")
        return None


def apply_speaker_diarization_to_transcription(segments_list, speaker_segments):
    """
    STT ê²°ê³¼ì— í™”ì ì •ë³´ ì ìš©
    
    Args:
        segments_list: STT ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        speaker_segments: í™”ì ë¶„ë¦¬ ê²°ê³¼
    
    Returns:
        list: í™”ì ì •ë³´ê°€ í¬í•¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not speaker_segments:
        return segments_list
    
    print("ğŸ”— STT ê²°ê³¼ì™€ í™”ì ì •ë³´ ì—°ê²° ì¤‘...")
    
    # í™”ì ì •ë³´ë¥¼ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
    speaker_timeline = []
    for speaker, segments in speaker_segments.items():
        for segment in segments:
            speaker_timeline.append({
                'speaker': speaker,
                'start': segment['start'],
                'end': segment['end']
            })
    
    speaker_timeline.sort(key=lambda x: x['start'])
    
    # STT ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì ì •ë³´ ë§¤í•‘
    enhanced_segments = []
    
    for segment in segments_list:
        segment_start = segment.start
        segment_end = segment.end
        
        # ê°€ì¥ ê²¹ì¹˜ëŠ” í™”ì ì°¾ê¸°
        best_speaker = "í™”ì1"  # ê¸°ë³¸ê°’
        max_overlap = 0
        
        for speaker_info in speaker_timeline:
            # ê²¹ì¹˜ëŠ” êµ¬ê°„ ê³„ì‚°
            overlap_start = max(segment_start, speaker_info['start'])
            overlap_end = min(segment_end, speaker_info['end'])
            overlap = max(0, overlap_end - overlap_start)
            
            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = speaker_info['speaker']
        
        # ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì ì •ë³´ ì¶”ê°€
        enhanced_segment = type('EnhancedSegment', (), {
            'start': segment.start,
            'end': segment.end,
            'text': segment.text,
            'speaker': best_speaker
        })()
        
        enhanced_segments.append(enhanced_segment)
    
    # í™”ìë³„ í†µê³„
    speaker_counts = {}
    for seg in enhanced_segments:
        speaker = seg.speaker
        speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1
    
    print("ğŸ“Š STT-í™”ì ë§¤í•‘ ì™„ë£Œ:")
    for speaker, count in speaker_counts.items():
        print(f"   {speaker}: {count}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    
    return enhanced_segments


def simple_time_based_diarization(segments_list, gap_threshold=5.0, max_speakers=4):
    """
    ê°„ë‹¨í•œ ì‹œê°„ ê¸°ë°˜ í™”ì êµ¬ë¶„ (ê¸°ì¡´ ë°©ì‹)
    
    Args:
        segments_list: STT ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸  
        gap_threshold: í™”ì ë³€ê²½ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ê³µë°± ì‹œê°„(ì´ˆ)
        max_speakers: ìµœëŒ€ í™”ì ìˆ˜
    
    Returns:
        list: í™”ì ì •ë³´ê°€ í¬í•¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ• ì‹œê°„ ê¸°ë°˜ ê°„ë‹¨ í™”ì êµ¬ë¶„ (ê³µë°± {gap_threshold}ì´ˆ ê¸°ì¤€)")
    
    current_speaker_num = 1
    enhanced_segments = []
    
    for i, segment in enumerate(segments_list):
        # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì™€ ê³µë°±ì´ í° ê²½ìš° í™”ì ë³€ê²½
        if i > 0:
            prev_segment = segments_list[i-1]
            gap = segment.start - prev_segment.end
            
            if gap > gap_threshold:
                current_speaker_num += 1
                if current_speaker_num > max_speakers:
                    current_speaker_num = 1
        
        speaker = f"í™”ì{current_speaker_num}"
        
        # ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì ì •ë³´ ì¶”ê°€
        enhanced_segment = type('EnhancedSegment', (), {
            'start': segment.start,
            'end': segment.end, 
            'text': segment.text,
            'speaker': speaker
        })()
        
        enhanced_segments.append(enhanced_segment)
    
    # í†µê³„
    speaker_counts = {}
    for seg in enhanced_segments:
        speaker = seg.speaker
        speaker_counts[speaker] = speaker_counts.get(speaker, 0) + 1
    
    print("ğŸ“Š ì‹œê°„ ê¸°ë°˜ í™”ì êµ¬ë¶„ ì™„ë£Œ:")
    for speaker, count in speaker_counts.items():
        print(f"   {speaker}: {count}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
    
    return enhanced_segments


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import sys
    
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python speaker_diarization.py <audio_file>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    
    if not os.path.exists(audio_file):
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file}")
        sys.exit(1)
    
    # í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸
    speaker_segments = perform_speaker_diarization(audio_file)
    
    if speaker_segments:
        print("\nğŸ­ í™”ì ë¶„ë¦¬ ê²°ê³¼:")
        for speaker, segments in speaker_segments.items():
            print(f"\n{speaker}:")
            for i, seg in enumerate(segments[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i+1}. {seg['start']:.1f}s - {seg['end']:.1f}s ({seg['duration']:.1f}s)")
            if len(segments) > 3:
                print(f"  ... ì´ {len(segments)}ê°œ êµ¬ê°„")
    else:
        print("í™”ì ë¶„ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œê°„ ê¸°ë°˜ êµ¬ë¶„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")